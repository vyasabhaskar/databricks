{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2543d1ac-c098-4fe0-b728-b3c164865081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from databricks_airline_performance_data.v01.flights_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a6061137-7c85-4e33-9cd1-a207469a2335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flights_small_df = spark.read.table(\"databricks_airline_performance_data.v01.flights_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4fb4a2c1-3039-45f0-be7a-1cc41a95eef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flights_small_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c56d112d-e5de-448d-b994-a08db7c0c25b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(flights_small_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "232f3b51-f046-4b76-a286-0865dbaa45b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# select only required columns from dataframe \n",
    "flights_small_required_cols_df = flights_small_df.select(\"Year\",\"Month\",\"DayofMonth\",\"DepTime\",\"FlightNum\",\"ActualElapsedTime\",\"CRSElapsedTime\",\"ArrDelay\")\n",
    "\n",
    "#get a initial Count \n",
    "initial_count = flights_small_required_cols_df.count()\n",
    "print(f\"selected record count , {initial_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e0f2cf2f-4bd4-4bb8-a928-63c9d4d7fb41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# examine the data for invalid values for string columns ArrDelay, ActualElapsedTime, DepTime\n",
    "# create a temporary view by casting three fields to integer\n",
    "\n",
    "flights_small_required_cols_df.selectExpr(\"Year\",\n",
    "                                      \"Month\",\n",
    "                                      \"DayofMonth\",\n",
    "                                      \"CAST(DepTime as INT) as DepTime\",\n",
    "                                      \"FlightNum\",\n",
    "                                      \"CAST(ActualElapsedTime AS INT) AS ActualElapsedTime\",\n",
    "                                      \"CRSElapsedTime\",\n",
    "                                      \"CAST(ArrDelay AS INT) AS ArrDelay\"\n",
    "                                      ).createOrReplaceTempView(\"flights_temp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cf847523-2352-4905-9d45-ea66dfbfdc48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW VIEWS IN global_temp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b523a599-0b4f-4cba-9bc2-1faa5b3700b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# use Spark SQL to count Nulls \n",
    "flights_small_required_cols_df.selectExpr(\"Year\",\n",
    "                                      \"Month\",\n",
    "                                      \"DayofMonth\",\n",
    "                                      \"try_cast(DepTime as INT) as DepTime\",\n",
    "                                      \"FlightNum\",\n",
    "                                      \"try_cast(ActualElapsedTime AS INT) AS ActualElapsedTime\",\n",
    "                                      \"CRSElapsedTime\",\n",
    "                                      \"try_cast(ArrDelay AS INT) AS ArrDelay\"\n",
    "                                      ).createOrReplaceTempView(\"flights_temp\")\n",
    "invalid_count_sql = spark.sql(\"\"\"\n",
    "                              select COUNT_IF(Year is null) as null_year_count,\n",
    "                                count_if(month is null) as null_month_count,\n",
    "                                count_if(dayofmonth is null) as null_dayofmonth_count,\n",
    "                                count_if(deptime is null) as null_deptime_count,\n",
    "                                count_if(FlightNum is null) as null_flightnum_count,\n",
    "                                count_if(ActualElapsedTime is null) as null_actualedelapsedtime_count,\n",
    "                                count_if(CRSElapsedTime is null) as null_CRSElapsedTime_count,\n",
    "                                count_if(arrdelay is null) as null_arrdelay_count\n",
    "                                from flights_temp\n",
    "                              \"\"\")\n",
    "display(invalid_count_sql)                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fb8d449c-5e8c-4a12-bd83-c6c4d48d8c85",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766238471220}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# counting invalid values using spark dataframe API code\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# use Spark SQL to count Nulls \n",
    "flights_small_required_cols_df.selectExpr(\"Year\",\n",
    "                                      \"Month\",\n",
    "                                      \"DayofMonth\",\n",
    "                                      \"try_cast(DepTime as INT) as DepTime\",\n",
    "                                      \"FlightNum\",\n",
    "                                      \"try_cast(ActualElapsedTime AS INT) AS ActualElapsedTime\",\n",
    "                                      \"CRSElapsedTime\",\n",
    "                                      \"try_cast(ArrDelay AS INT) AS ArrDelay\"\n",
    "                                      ).createOrReplaceTempView(\"flights_temp\")\n",
    "\n",
    "\n",
    "flights_invalid_df = spark.table(\"flights_temp\")\n",
    "\n",
    "# use dataframe API to count invalid values\n",
    "invalid_count_df = flights_invalid_df.select(    \n",
    "    sum(when(col(\"Year\").isNull(), 1).otherwise(0)).alias(\"null_year_count\"),\n",
    "    sum(when(col('Month').isNull(), 1).otherwise(0)).alias(\"null_month_count\"),\n",
    "    sum(when(col('DayofMonth').isNull(), 1).otherwise(0)).alias(\"null_dayofmonth_count\"),\n",
    "    sum(when(col('DepTime').isNull(), 1).otherwise(0)).alias(\"null_deptime_count\"),\n",
    "    sum(when(col('FlightNum').isNull(), 1).otherwise(0)).alias(\"null_flightnum_count\"),\n",
    "    sum(when(col('ActualElapsedTime').isNull(), 1).otherwise(0)).alias(\"null_actualedelapsedtime_count\"),\n",
    "    sum(when(col('CRSElapsedTime').isNull(), 1).otherwise(0)).alias(\"null_crseelapsedtime_count\"),\n",
    "    sum(when(col('ArrDelay').isNull(), 1).otherwise(0)).alias(\"null_arrdelay_count\")\n",
    ")\n",
    "\n",
    "display(invalid_count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c2b41f8c-9d8a-4f92-9e4a-5a63c9fe8b28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "invalid_count_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cd928434-6c5d-4568-b7c1-e612026fce55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "invalid_count_pd = invalid_count_df.toPandas()\n",
    "\n",
    "# Prepare data for bar chart\n",
    "columns = invalid_count_pd.columns\n",
    "null_counts = invalid_count_pd.iloc[0].values\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(columns, null_counts, color='skyblue')\n",
    "plt.xlabel('Column')\n",
    "plt.ylabel('Null Count')\n",
    "plt.title('Null Counts per Column')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "923c5379-3a6c-4956-8657-24449661ff57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "not_null_flights_df = flights_small_required_cols_df.na.drop(how=\"any\",subset=['CRSElapsedTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d3853d84-6e02-4c09-beb1-47facee4ed6c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766248475805}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flights_with_valid_data_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9a82e6a-d04c-4b4b-99e7-7efacbd274c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# delete rows with invalid data in ArrDelay, ActualElapsedDate & DepTime columns\n",
    "flights_with_valid_data_df = not_null_flights_df.filter(\n",
    "    col(\"ArrDelay\").try_cast(\"Integer\").isNotNull() &\n",
    "    col(\"ActualElapsedTime\").try_cast(\"Integer\").isNotNull() &\n",
    "    col(\"DepTime\").try_cast(\"Integer\").isNotNull() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d91d455f-a21e-4d89-843e-83ff3d9f2393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# now cast the fields to integer\n",
    "clean_flights_data_df = flights_with_valid_data_df.withColumn(\"ArrDelay\",col(\"ArrDelay\").cast(\"Integer\")).withColumn(\"ActualElapsedTime\",col(\"ActualElapsedTime\").cast(\"Integer\"))\n",
    "clean_flights_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "169027e9-5a8c-44f8-986c-8368869ec631",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766249380511}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(clean_flights_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "20d55f74-8056-4fb2-81ef-56575fedebef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Enrichment\n",
    "# Concatenate Year, Month and DayofMonth columns to create a new column called Date Timestamp\n",
    "\n",
    "from pyspark.sql.functions import col,make_timestamp_ntz,lpad, substr,lit\n",
    "\n",
    "flights_with_datetime_df = clean_flights_data_df.withColumn(\n",
    "    \"FlightDateTime\", \n",
    "    make_timestamp_ntz(\n",
    "        col(\"year\"),\n",
    "        col(\"Month\"),\n",
    "        col(\"DayofMonth\"),\n",
    "        substr(lpad(col(\"DepTime\"),4,\"0\"),lit(1),lit(2)).cast(\"integer\"),\n",
    "        substr(lpad(col(\"DepTime\"),4,\"0\"),lit(3),lit(2)).cast(\"integer\"),\n",
    "        lit(0)\n",
    "        )\n",
    "    ).drop(\"Year\",\"Month\",\"DayofMonth\",\"DepTime\")\n",
    "\n",
    "# show the result\n",
    "display(flights_with_datetime_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5d1a3f4b-5674-42f1-b38e-28d2dc303e59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# finding elapsed date time from \n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "flights_elapsed_time_df = flights_with_datetime_df.withColumn(\"ElapsedTimeDiff\", col(\"ActualElapsedTime\")-col(\"CRSElapsedTime\")).drop(\"ActualElapsedTime\",\"CRSElapsedTime\")\n",
    "\n",
    "display(flights_elapsed_time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69309625-799f-43c5-9365-d7e65a6f6b11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# calculate delay category based on ArrDelay time\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "flight_delay_category_df = flights_elapsed_time_df.withColumn(\"delay_category\",\n",
    "                                                        when(col(\"ArrDelay\")<=0, \"On Time\").\n",
    "                                                        when(col(\"ArrDelay\")<=15 , \"Slight Delay\").\n",
    "                                                        when(col(\"ArrDelay\")<=60, \"Moderate Delay\").\n",
    "                                                        otherwise(\"SevereDelay\")\n",
    "                                                        ). drop(\"ArrDelay\")\n",
    "display(flight_delay_category_df)                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1e2e803d-8084-4adc-a8aa-d69b0689b7c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# using UDF functions, calculate Z score (standard deviation and mean)\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def normalized_diff(diff_series):\n",
    "    return (diff_series - diff_series.mean()) / diff_series.std()\n",
    "\n",
    "udf_example = flight_delay_category_df.withColumn(\"normalized_diff\", normalized_diff(\"ElapsedTimeDiff\"))\n",
    "\n",
    "display(udf_example)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5203546392077185,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL Enrichments.",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
